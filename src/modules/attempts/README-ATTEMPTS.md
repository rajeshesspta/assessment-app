# Attempts Module Notes

Attempts capture a learner’s responses for a specific assessment assignment. In practice, Tenant Admins or Content Authors assign assessments to cohorts of learners, and each learner launches an attempt scoped to their tenant + assessment + user id.

Partition key candidates: /tenantId#assessmentId#userId or shard bucket for high concurrency.
Responses may embed if total size < 2MB else split into container keyed by attemptId. Short-answer and essay items keep attempts in `submitted` status and require external evaluators (manual or AI rubric) to publish scores after consuming the `FreeResponseEvaluationRequested` events. Scenario/coding tasks submit `scenarioAnswer` payloads (repository URL, artifact URL, submission notes, and optional supporting files) and emit `ScenarioEvaluationRequested` events so automation services can grade before scores are persisted; these attempts also remain `submitted` until processing completes. Numeric entry responses use `numericAnswer.value` (with optional `unit`) and are auto-evaluated server-side using either exact-with-tolerance or range validation rules defined on the item. Hotspot/image-region interactions send an array of normalized `{ x, y }` coordinates via `hotspotAnswers`; the API clamps the coordinates to `[0, 1]`, runs a point-in-polygon test against the stored regions, and scores either all-or-nothing or per-region partial credit depending on the item’s scoring rules. Drag-and-drop activities post `dragDropAnswers` arrays, each element mapping a `tokenId` to a `dropZoneId` (plus optional `position` for ordered zones); submission logic enforces per-zone token budgets, honors ordered evaluations, and awards credit according to the item’s `scoring.mode` (`all`, `per_zone`, or `per_token`).
